#### Original script from Lea Schmidt ####

# Complete pipeline: fMRIPrep preprocessed BOLD to Functional Connectivity Matrix
# This replicates much of what XCP-D does but with full manual control
#%%
import numpy as np
import pandas as pd
import nibabel as nib
import matplotlib.pyplot as plt
from nilearn import datasets, input_data, plotting, image, signal
from nilearn.connectome import ConnectivityMeasure
from nilearn.image import clean_img, smooth_img
from scipy import stats
import os
import warnings
warnings.filterwarnings('ignore')
import glob
import re

def bold_to_fc_complete_pipeline(
    bold_file,
    confounds_file,
    atlas_name='schaefer_400',
    custom_atlas_file=None,
    custom_atlas_labels=None,
    smoothing_fwhm=6,
    low_pass=0.1,
    high_pass=0.01,
    t_r=2.0,
    fd_threshold=0.5,
    dvars_threshold=2.0,
    denoising_strategy='24P+8PhysioCor+SpikeReg',
    standardize=True,
    connectivity_kind='correlation',
    plot_results=True,
    output_dir='./fc_results',
    subject_id = 'XX'
):
    """
    Complete pipeline from fMRIPrep preprocessed BOLD to FC matrix
    Replicates XCP-D functionality with manual control
    
    Parameters:
    -----------
    bold_file : str
        Path to preprocessed BOLD file (*_desc-preproc_bold.nii.gz)
    confounds_file : str  
        Path to confounds file (*_desc-confounds_timeseries.tsv)
    atlas_name : str
        Atlas to use ('schaefer_400', 'schaefer_200', 'aal', 'power_264', 'custom')
    custom_atlas_file : str, optional
        Path to custom atlas NIfTI file (use when atlas_name='custom')
    custom_atlas_labels : list, optional  
        List of region labels for custom atlas
    smoothing_fwhm : float
        FWHM for spatial smoothing in mm
    low_pass : float
        Low-pass filter frequency in Hz
    high_pass : float  
        High-pass filter frequency in Hz
    t_r : float
        Repetition time in seconds
    fd_threshold : float
        Framewise displacement threshold for scrubbing (mm)
    dvars_threshold : float
        DVARS threshold for scrubbing
    denoising_strategy : str
        Denoising strategy ('6P', '24P', '24P+8PhysioCor+SpikeReg', 'AROMA+GSR')
    standardize : bool
        Whether to standardize time series
    connectivity_kind : str
        Type of connectivity ('correlation', 'partial correlation', 'covariance')
    plot_results : bool
        Whether to generate plots
    output_dir : str
        Output directory for results
    """
    
    print("=== BOLD to FC Matrix Pipeline ===")
    print(f"Processing: {os.path.basename(bold_file)}")
    
    # Create output directory
    os.makedirs(output_dir, exist_ok=True)
    
    # ==========================================
    # 1. LOAD DATA
    # ==========================================
    print("\n1. Loading data...")
    bold_img = nib.load(bold_file)
    confounds_df = pd.read_csv(confounds_file, sep='\t')
    
    print(f"   BOLD shape: {bold_img.shape}")
    print(f"   Confounds shape: {confounds_df.shape}")
    
    # ==========================================
    # 2. LOAD ATLAS
    # ==========================================
    print(f"\n2. Loading atlas: {atlas_name}...")
    if atlas_name == 'custom':
        if custom_atlas_file is None:
            raise ValueError("custom_atlas_file must be provided when atlas_name='custom'")
        
        print(f"   Loading custom atlas: {custom_atlas_file}")
        atlas_img = nib.load(custom_atlas_file)
        
        # Get unique labels from atlas (excluding background=0)
        atlas_data = atlas_img.get_fdata()
        unique_labels = np.unique(atlas_data)[1:]  # Exclude 0 (background)
        n_regions = len(unique_labels)
        
        # Use provided labels or create default ones
        if custom_atlas_labels is not None:
            if len(custom_atlas_labels) != n_regions:
                print(f"   Warning: Provided {len(custom_atlas_labels)} labels but atlas has {n_regions} regions")
                labels = [f"Region_{i:03d}" for i in unique_labels]
            else:
                labels = custom_atlas_labels
        else:
            labels = [f"Region_{int(i):03d}" for i in unique_labels]
        
        print(f"   Custom atlas loaded: {n_regions} regions")
        print(f"   Atlas shape: {atlas_img.shape}")
        print(f"   Unique labels: {unique_labels[:10]}{'...' if len(unique_labels) > 10 else ''}")
        
    elif atlas_name == 'schaefer_400':
        atlas = datasets.fetch_atlas_schaefer_2018(n_rois=400, yeo_networks=7, resolution_mm=2)
        atlas_img = atlas.maps
        labels = atlas.labels
    elif atlas_name == 'schaefer_200':
        atlas = datasets.fetch_atlas_schaefer_2018(n_rois=200, yeo_networks=7, resolution_mm=2)
        atlas_img = atlas.maps
        labels = atlas.labels
    elif atlas_name == 'aal':
        atlas = datasets.fetch_atlas_aal(version='SPM12')
        atlas_img = atlas.maps
        labels = atlas.labels
    elif atlas_name == 'power_264':
        atlas = datasets.fetch_coords_power_2011()
        # For Power atlas, we'll use spheres
        atlas_img = None
        coords = np.vstack((atlas.rois['x'], atlas.rois['y'], atlas.rois['z'])).T
        labels = [f"Power_{i:03d}" for i in range(len(coords))]
    else:
        raise ValueError(f"Unknown atlas: {atlas_name}")
    
    if atlas_img is not None:
        print(f"   Atlas loaded with {len(labels)} regions")
    else:
        print(f"   Coordinate-based atlas loaded with {len(coords)} regions")
    
    # ==========================================
    # 3. PREPARE CONFOUNDS
    # ==========================================
    print(f"\n3. Preparing confounds with strategy: {denoising_strategy}...")
    
    # Base motion parameters (6P)
    motion_params = ['trans_x', 'trans_y', 'trans_z', 'rot_x', 'rot_y', 'rot_z']
    
    confound_vars = []
    
    if '6P' in denoising_strategy:
        confound_vars.extend(motion_params)

    if '12P' in denoising_strategy:
    # 6 motion + 6 derivatives + 12 squared terms
        motion_derivs = [f"{param}_derivative1" for param in motion_params]
        
        confound_vars.extend(motion_params)
        confound_vars.extend([col for col in motion_derivs if col in confounds_df.columns])

    if '24P' in denoising_strategy:
        # 6 motion + 6 derivatives + 12 squared terms
        motion_derivs = [f"{param}_derivative1" for param in motion_params]
        motion_squared = [f"{param}_power2" for param in motion_params]
        motion_deriv_squared = [f"{param}_derivative1_power2" for param in motion_params]
        
        confound_vars.extend(motion_params)
        confound_vars.extend([col for col in motion_derivs if col in confounds_df.columns])
        confound_vars.extend([col for col in motion_squared if col in confounds_df.columns])
        confound_vars.extend([col for col in motion_deriv_squared if col in confounds_df.columns])
    
    if 'PhysioCor' in denoising_strategy:
        # Add CompCor components
        compcor_cols = [col for col in confounds_df.columns if 'a_comp_cor_' in col][:8]
        confound_vars.extend(compcor_cols)
        
        # Add physiological signals (WITHOUT global signal for no-GSR analysis)
        physio_signals = ['csf', 'white_matter']
        # Only add global signal if explicitly requested
        if 'GSR' in denoising_strategy or 'global_signal' in denoising_strategy.lower():
            physio_signals.append('global_signal')
        
        confound_vars.extend([col for col in physio_signals if col in confounds_df.columns])
    
    if 'AROMA' in denoising_strategy:
        # Add AROMA components if available
        aroma_cols = [col for col in confounds_df.columns if 'aroma_motion_' in col]
        confound_vars.extend(aroma_cols)
    
    # Ensure all confound variables exist
    confound_vars = [col for col in confound_vars if col in confounds_df.columns]
    confounds = confounds_df[confound_vars].fillna(0)
    
    print(f"   Using {len(confound_vars)} confound regressors:")
    print(f"   {confound_vars}")
    
    # ==========================================
    # 4. IDENTIFY OUTLIER TIMEPOINTS
    # ==========================================
    print(f"\n4. Identifying outlier timepoints...")
    
    outlier_mask = np.zeros(len(confounds_df), dtype=bool)
    
    # Framewise displacement
    if 'framewise_displacement' in confounds_df.columns:
        fd_outliers = confounds_df['framewise_displacement'] > fd_threshold
        outlier_mask |= fd_outliers.fillna(False)
        print(f"   FD outliers (>{fd_threshold}mm): {fd_outliers.sum()}")

    if outlier_mask.sum()/len(outlier_mask) > 0.2:
        print(f"Too many FD outliers: {outlier_mask.sum()/len(outlier_mask):.1%} â€” stopping function.")
        return
# DVARS
    
    # DVARS
    if 'std_dvars' in confounds_df.columns:
        dvars_outliers = confounds_df['std_dvars'] > dvars_threshold
        outlier_mask |= dvars_outliers.fillna(False)
        print(f"   DVARS outliers (>{dvars_threshold}): {dvars_outliers.sum()}")
    
    print(f"   Total outlier timepoints: {outlier_mask.sum()}/{len(outlier_mask)}")
    
    # ==========================================
    # 5. SPATIAL SMOOTHING
    # ==========================================
    if smoothing_fwhm > 0:
        print(f"\n5. Applying spatial smoothing (FWHM={smoothing_fwhm}mm)...")
        bold_img = smooth_img(bold_img, fwhm=smoothing_fwhm)
    else:
        print("\n5. Skipping spatial smoothing...")
    
    # ==========================================
    # 6. EXTRACT TIME SERIES
    # ==========================================
    print(f"\n6. Extracting time series...")
    
    if atlas_img is not None:
        # Use atlas-based masker
        masker = input_data.NiftiLabelsMasker(
            labels_img=atlas_img,
            standardize=False,  # We'll handle standardization later
            memory='nilearn_cache',
            verbose=0
        )
    else:
        # Use coordinates-based masker (e.g., Power atlas)
        masker = input_data.NiftiSpheresMasker(
            seeds=coords,
            radius=5,
            standardize=False,
            memory='nilearn_cache',
            verbose=0
        )
    
    # Extract raw time series
    time_series = masker.fit_transform(bold_img)
    print(f"   Extracted time series shape: {time_series.shape}")
    
    # Check for problematic regions (all zeros, constant values, etc.)
    problematic_regions = []
    for i in range(time_series.shape[1]):
        region_ts = time_series[:, i]
        if np.all(region_ts == 0):
            problematic_regions.append(i)
            print(f"   Warning: Region {i} has all-zero time series")
        elif np.std(region_ts) < 1e-10:
            problematic_regions.append(i)
            print(f"   Warning: Region {i} has constant time series (std={np.std(region_ts):.2e})")
        elif np.any(np.isnan(region_ts)):
            problematic_regions.append(i)
            print(f"   Warning: Region {i} contains NaN values")
    
    if problematic_regions:
        print(f"   Found {len(problematic_regions)} problematic regions out of {time_series.shape[1]}")
        # Remove problematic regions
        good_regions = [i for i in range(time_series.shape[1]) if i not in problematic_regions]
        if len(good_regions) < 10:
            raise ValueError(f"Too few valid regions ({len(good_regions)}). Check your atlas alignment.")
        
        time_series = time_series[:, good_regions]
        labels = [labels[i] for i in good_regions]
        print(f"   Kept {len(good_regions)} valid regions for analysis")
    
    # ==========================================
    # 7. TEMPORAL CLEANING
    # ==========================================
    print(f"\n7. Temporal filtering and denoising...")
    
    # Prepare spike regressors for outlier timepoints
    spike_regressors = None
    if 'SpikeReg' in denoising_strategy and outlier_mask.sum() > 0:
        spike_regressors = np.zeros((len(outlier_mask), outlier_mask.sum()))
        spike_regressors[outlier_mask, :] = np.eye(outlier_mask.sum())
        print(f"   Adding {spike_regressors.shape[1]} spike regressors")
    
    # Combine confounds with spike regressors
    all_confounds = confounds.values
    if spike_regres...
