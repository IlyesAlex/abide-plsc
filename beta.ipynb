{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "455bb4cc",
   "metadata": {},
   "source": [
    "### Original code from LÃ©a Schmidt ###\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e89cd1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Complete pipeline: fMRIPrep preprocessed BOLD to Functional Connectivity Matrix\n",
    "# This replicates much of what XCP-D does but with full manual control\n",
    "#%%\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "from nilearn import datasets, input_data, plotting, image, signal\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "from nilearn.image import clean_img, smooth_img\n",
    "from scipy import stats\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import glob\n",
    "import re\n",
    "\n",
    "def bold_to_fc_complete_pipeline(\n",
    "    bold_file,\n",
    "    confounds_file,\n",
    "    atlas_name='schaefer_400',\n",
    "    custom_atlas_file=None,\n",
    "    custom_atlas_labels=None,\n",
    "    smoothing_fwhm=6,\n",
    "    low_pass=0.1,\n",
    "    high_pass=0.01,\n",
    "    t_r=2.0,\n",
    "    fd_threshold=0.5,\n",
    "    dvars_threshold=2.0,\n",
    "    denoising_strategy='24P+8PhysioCor+SpikeReg',\n",
    "    standardize=True,\n",
    "    connectivity_kind='correlation',\n",
    "    plot_results=True,\n",
    "    output_dir='./fc_results',\n",
    "    subject_id = 'XX'\n",
    "):\n",
    "    \"\"\"\n",
    "    Complete pipeline from fMRIPrep preprocessed BOLD to FC matrix\n",
    "    Replicates XCP-D functionality with manual control\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    bold_file : str\n",
    "        Path to preprocessed BOLD file (*_desc-preproc_bold.nii.gz)\n",
    "    confounds_file : str  \n",
    "        Path to confounds file (*_desc-confounds_timeseries.tsv)\n",
    "    atlas_name : str\n",
    "        Atlas to use ('schaefer_400', 'schaefer_200', 'aal', 'power_264', 'custom')\n",
    "    custom_atlas_file : str, optional\n",
    "        Path to custom atlas NIfTI file (use when atlas_name='custom')\n",
    "    custom_atlas_labels : list, optional  \n",
    "        List of region labels for custom atlas\n",
    "    smoothing_fwhm : float\n",
    "        FWHM for spatial smoothing in mm\n",
    "    low_pass : float\n",
    "        Low-pass filter frequency in Hz\n",
    "    high_pass : float  \n",
    "        High-pass filter frequency in Hz\n",
    "    t_r : float\n",
    "        Repetition time in seconds\n",
    "    fd_threshold : float\n",
    "        Framewise displacement threshold for scrubbing (mm)\n",
    "    dvars_threshold : float\n",
    "        DVARS threshold for scrubbing\n",
    "    denoising_strategy : str\n",
    "        Denoising strategy ('6P', '24P', '24P+8PhysioCor+SpikeReg', 'AROMA+GSR')\n",
    "    standardize : bool\n",
    "        Whether to standardize time series\n",
    "    connectivity_kind : str\n",
    "        Type of connectivity ('correlation', 'partial correlation', 'covariance')\n",
    "    plot_results : bool\n",
    "        Whether to generate plots\n",
    "    output_dir : str\n",
    "        Output directory for results\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== BOLD to FC Matrix Pipeline ===\")\n",
    "    print(f\"Processing: {os.path.basename(bold_file)}\")\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # ==========================================\n",
    "    # 1. LOAD DATA\n",
    "    # ==========================================\n",
    "    print(\"\\n1. Loading data...\")\n",
    "    bold_img = nib.load(bold_file)\n",
    "    confounds_df = pd.read_csv(confounds_file, sep='\\t')\n",
    "    \n",
    "    print(f\"   BOLD shape: {bold_img.shape}\")\n",
    "    print(f\"   Confounds shape: {confounds_df.shape}\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # 2. LOAD ATLAS\n",
    "    # ==========================================\n",
    "    print(f\"\\n2. Loading atlas: {atlas_name}...\")\n",
    "    if atlas_name == 'custom':\n",
    "        if custom_atlas_file is None:\n",
    "            raise ValueError(\"custom_atlas_file must be provided when atlas_name='custom'\")\n",
    "        \n",
    "        print(f\"   Loading custom atlas: {custom_atlas_file}\")\n",
    "        atlas_img = nib.load(custom_atlas_file)\n",
    "        \n",
    "        # Get unique labels from atlas (excluding background=0)\n",
    "        atlas_data = atlas_img.get_fdata()\n",
    "        unique_labels = np.unique(atlas_data)[1:]  # Exclude 0 (background)\n",
    "        n_regions = len(unique_labels)\n",
    "        \n",
    "        # Use provided labels or create default ones\n",
    "        if custom_atlas_labels is not None:\n",
    "            if len(custom_atlas_labels) != n_regions:\n",
    "                print(f\"   Warning: Provided {len(custom_atlas_labels)} labels but atlas has {n_regions} regions\")\n",
    "                labels = [f\"Region_{i:03d}\" for i in unique_labels]\n",
    "            else:\n",
    "                labels = custom_atlas_labels\n",
    "        else:\n",
    "            labels = [f\"Region_{int(i):03d}\" for i in unique_labels]\n",
    "        \n",
    "        print(f\"   Custom atlas loaded: {n_regions} regions\")\n",
    "        print(f\"   Atlas shape: {atlas_img.shape}\")\n",
    "        print(f\"   Unique labels: {unique_labels[:10]}{'...' if len(unique_labels) > 10 else ''}\")\n",
    "        \n",
    "    elif atlas_name == 'schaefer_400':\n",
    "        atlas = datasets.fetch_atlas_schaefer_2018(n_rois=400, yeo_networks=7, resolution_mm=2)\n",
    "        atlas_img = atlas.maps\n",
    "        labels = atlas.labels\n",
    "    elif atlas_name == 'schaefer_200':\n",
    "        atlas = datasets.fetch_atlas_schaefer_2018(n_rois=200, yeo_networks=7, resolution_mm=2)\n",
    "        atlas_img = atlas.maps\n",
    "        labels = atlas.labels\n",
    "    elif atlas_name == 'aal':\n",
    "        atlas = datasets.fetch_atlas_aal(version='SPM12')\n",
    "        atlas_img = atlas.maps\n",
    "        labels = atlas.labels\n",
    "    elif atlas_name == 'power_264':\n",
    "        atlas = datasets.fetch_coords_power_2011()\n",
    "        # For Power atlas, we'll use spheres\n",
    "        atlas_img = None\n",
    "        coords = np.vstack((atlas.rois['x'], atlas.rois['y'], atlas.rois['z'])).T\n",
    "        labels = [f\"Power_{i:03d}\" for i in range(len(coords))]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown atlas: {atlas_name}\")\n",
    "    \n",
    "    if atlas_img is not None:\n",
    "        print(f\"   Atlas loaded with {len(labels)} regions\")\n",
    "    else:\n",
    "        print(f\"   Coordinate-based atlas loaded with {len(coords)} regions\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # 3. PREPARE CONFOUNDS\n",
    "    # ==========================================\n",
    "    print(f\"\\n3. Preparing confounds with strategy: {denoising_strategy}...\")\n",
    "    \n",
    "    # Base motion parameters (6P)\n",
    "    motion_params = ['trans_x', 'trans_y', 'trans_z', 'rot_x', 'rot_y', 'rot_z']\n",
    "    \n",
    "    confound_vars = []\n",
    "    \n",
    "    if '6P' in denoising_strategy:\n",
    "        confound_vars.extend(motion_params)\n",
    "\n",
    "    if '12P' in denoising_strategy:\n",
    "    # 6 motion + 6 derivatives + 12 squared terms\n",
    "        motion_derivs = [f\"{param}_derivative1\" for param in motion_params]\n",
    "        \n",
    "        confound_vars.extend(motion_params)\n",
    "        confound_vars.extend([col for col in motion_derivs if col in confounds_df.columns])\n",
    "\n",
    "    if '24P' in denoising_strategy:\n",
    "        # 6 motion + 6 derivatives + 12 squared terms\n",
    "        motion_derivs = [f\"{param}_derivative1\" for param in motion_params]\n",
    "        motion_squared = [f\"{param}_power2\" for param in motion_params]\n",
    "        motion_deriv_squared = [f\"{param}_derivative1_power2\" for param in motion_params]\n",
    "        \n",
    "        confound_vars.extend(motion_params)\n",
    "        confound_vars.extend([col for col in motion_derivs if col in confounds_df.columns])\n",
    "        confound_vars.extend([col for col in motion_squared if col in confounds_df.columns])\n",
    "        confound_vars.extend([col for col in motion_deriv_squared if col in confounds_df.columns])\n",
    "    \n",
    "    if 'PhysioCor' in denoising_strategy:\n",
    "        # Add CompCor components\n",
    "        compcor_cols = [col for col in confounds_df.columns if 'a_comp_cor_' in col][:8]\n",
    "        confound_vars.extend(compcor_cols)\n",
    "        \n",
    "        # Add physiological signals (WITHOUT global signal for no-GSR analysis)\n",
    "        physio_signals = ['csf', 'white_matter']\n",
    "        # Only add global signal if explicitly requested\n",
    "        if 'GSR' in denoising_strategy or 'global_signal' in denoising_strategy.lower():\n",
    "            physio_signals.append('global_signal')\n",
    "        \n",
    "        confound_vars.extend([col for col in physio_signals if col in confounds_df.columns])\n",
    "    \n",
    "    if 'AROMA' in denoising_strategy:\n",
    "        # Add AROMA components if available\n",
    "        aroma_cols = [col for col in confounds_df.columns if 'aroma_motion_' in col]\n",
    "        confound_vars.extend(aroma_cols)\n",
    "    \n",
    "    # Ensure all confound variables exist\n",
    "    confound_vars = [col for col in confound_vars if col in confounds_df.columns]\n",
    "    confounds = confounds_df[confound_vars].fillna(0)\n",
    "    \n",
    "    print(f\"   Using {len(confound_vars)} confound regressors:\")\n",
    "    print(f\"   {confound_vars}\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # 4. IDENTIFY OUTLIER TIMEPOINTS\n",
    "    # ==========================================\n",
    "    print(f\"\\n4. Identifying outlier timepoints...\")\n",
    "    \n",
    "    outlier_mask = np.zeros(len(confounds_df), dtype=bool)\n",
    "    \n",
    "    # Framewise displacement\n",
    "    if 'framewise_displacement' in confounds_df.columns:\n",
    "        fd_outliers = confounds_df['framewise_displacement'] > fd_threshold\n",
    "        outlier_mask |= fd_outliers.fillna(False)\n",
    "        print(f\"   FD outliers (>{fd_threshold}mm): {fd_outliers.sum()}\")\n",
    "\n",
    "    if outlier_mask.sum()/len(outlier_mask) > 0.2:\n",
    "        print(f\"Too many FD outliers: {outlier_mask.sum()/len(outlier_mask):.1%} â stopping function.\")\n",
    "        return\n",
    "# DVARS\n",
    "    \n",
    "    # DVARS\n",
    "    if 'std_dvars' in confounds_df.columns:\n",
    "        dvars_outliers = confounds_df['std_dvars'] > dvars_threshold\n",
    "        outlier_mask |= dvars_outliers.fillna(False)\n",
    "        print(f\"   DVARS outliers (>{dvars_threshold}): {dvars_outliers.sum()}\")\n",
    "    \n",
    "    print(f\"   Total outlier timepoints: {outlier_mask.sum()}/{len(outlier_mask)}\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # 5. SPATIAL SMOOTHING\n",
    "    # ==========================================\n",
    "    if smoothing_fwhm > 0:\n",
    "        print(f\"\\n5. Applying spatial smoothing (FWHM={smoothing_fwhm}mm)...\")\n",
    "        bold_img = smooth_img(bold_img, fwhm=smoothing_fwhm)\n",
    "    else:\n",
    "        print(\"\\n5. Skipping spatial smoothing...\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # 6. EXTRACT TIME SERIES\n",
    "    # ==========================================\n",
    "    print(f\"\\n6. Extracting time series...\")\n",
    "    \n",
    "    if atlas_img is not None:\n",
    "        # Use atlas-based masker\n",
    "        masker = input_data.NiftiLabelsMasker(\n",
    "            labels_img=atlas_img,\n",
    "            standardize=False,  # We'll handle standardization later\n",
    "            memory='nilearn_cache',\n",
    "            verbose=0\n",
    "        )\n",
    "    else:\n",
    "        # Use coordinates-based masker (e.g., Power atlas)\n",
    "        masker = input_data.NiftiSpheresMasker(\n",
    "            seeds=coords,\n",
    "            radius=5,\n",
    "            standardize=False,\n",
    "            memory='nilearn_cache',\n",
    "            verbose=0\n",
    "        )\n",
    "    \n",
    "    # Extract raw time series\n",
    "    time_series = masker.fit_transform(bold_img)\n",
    "    print(f\"   Extracted time series shape: {time_series.shape}\")\n",
    "    \n",
    "    # Check for problematic regions (all zeros, constant values, etc.)\n",
    "    problematic_regions = []\n",
    "    for i in range(time_series.shape[1]):\n",
    "        region_ts = time_series[:, i]\n",
    "        if np.all(region_ts == 0):\n",
    "            problematic_regions.append(i)\n",
    "            print(f\"   Warning: Region {i} has all-zero time series\")\n",
    "        elif np.std(region_ts) < 1e-10:\n",
    "            problematic_regions.append(i)\n",
    "            print(f\"   Warning: Region {i} has constant time series (std={np.std(region_ts):.2e})\")\n",
    "        elif np.any(np.isnan(region_ts)):\n",
    "            problematic_regions.append(i)\n",
    "            print(f\"   Warning: Region {i} contains NaN values\")\n",
    "    \n",
    "    if problematic_regions:\n",
    "        print(f\"   Found {len(problematic_regions)} problematic regions out of {time_series.shape[1]}\")\n",
    "        # Remove problematic regions\n",
    "        good_regions = [i for i in range(time_series.shape[1]) if i not in problematic_regions]\n",
    "        if len(good_regions) < 10:\n",
    "            raise ValueError(f\"Too few valid regions ({len(good_regions)}). Check your atlas alignment.\")\n",
    "        \n",
    "        time_series = time_series[:, good_regions]\n",
    "        labels = [labels[i] for i in good_regions]\n",
    "        print(f\"   Kept {len(good_regions)} valid regions for analysis\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # 7. TEMPORAL CLEANING\n",
    "    # ==========================================\n",
    "    print(f\"\\n7. Temporal filtering and denoising...\")\n",
    "    \n",
    "    # Prepare spike regressors for outlier timepoints\n",
    "    spike_regressors = None\n",
    "    if 'SpikeReg' in denoising_strategy and outlier_mask.sum() > 0:\n",
    "        spike_regressors = np.zeros((len(outlier_mask), outlier_mask.sum()))\n",
    "        spike_regressors[outlier_mask, :] = np.eye(outlier_mask.sum())\n",
    "        print(f\"   Adding {spike_regressors.shape[1]} spike regressors\")\n",
    "    \n",
    "    # Combine confounds with spike regressors\n",
    "    all_confounds = confounds.values\n",
    "    if spike_regressors is not None:\n",
    "        all_confounds = np.hstack([all_confounds, spike_regressors])\n",
    "    \n",
    "    # Apply temporal filtering and confound regression\n",
    "    time_series_clean = signal.clean(\n",
    "        time_series,\n",
    "        confounds=all_confounds,\n",
    "        t_r=t_r,\n",
    "        low_pass=low_pass,\n",
    "        high_pass=high_pass,\n",
    "        detrend=True,\n",
    "        standardize=standardize\n",
    "    )\n",
    "    \n",
    "    print(f\"   Cleaned time series shape: {time_series_clean.shape}\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # 8. CENSORING (INTERPOLATION)\n",
    "    # ==========================================\n",
    "    print(f\"\\n8. Handling censored timepoints...\")\n",
    "    \n",
    "    # For connectivity analysis, we typically interpolate rather than remove timepoints\n",
    "    if outlier_mask.sum() > 0:\n",
    "        print(f\"   Interpolating {outlier_mask.sum()} outlier timepoints...\")\n",
    "        time_series_final = time_series_clean.copy()\n",
    "        \n",
    "        # Simple linear interpolation for outlier timepoints\n",
    "        for i, is_outlier in enumerate(outlier_mask):\n",
    "            if is_outlier:\n",
    "                # Find nearest non-outlier timepoints\n",
    "                before = i - 1\n",
    "                after = i + 1\n",
    "                while before >= 0 and outlier_mask[before]:\n",
    "                    before -= 1\n",
    "                while after < len(outlier_mask) and outlier_mask[after]:\n",
    "                    after += 1\n",
    "                \n",
    "                if before >= 0 and after < len(outlier_mask):\n",
    "                    # Linear interpolation\n",
    "                    alpha = (i - before) / (after - before)\n",
    "                    time_series_final[i] = ((1 - alpha) * time_series_clean[before] + \n",
    "                                          alpha * time_series_clean[after])\n",
    "                elif before >= 0:\n",
    "                    time_series_final[i] = time_series_clean[before]\n",
    "                elif after < len(outlier_mask):\n",
    "                    time_series_final[i] = time_series_clean[after]\n",
    "    else:\n",
    "        time_series_final = time_series_clean\n",
    "\n",
    "\n",
    "    # ==========================================\n",
    "    # Setp 8.1 -> Save the nifti image\n",
    "    # ==========================================\n",
    "\n",
    "    # print(f\"   Saving fully cleaned BOLD image...\")\n",
    "    # cleaned_bold_img = image.new_img_like(bold_img, time_series_final.T.reshape(bold_img.shape))\n",
    "    # cleaned_bold_path = os.path.join('/media/leas/Elements/PhD/PNC_Data/rsfmri_preprocessed_nilearn/', f'{subject_id}_cleaned_bold_interpolated.nii.gz')\n",
    "    # cleaned_bold_img.to_filename(cleaned_bold_path)\n",
    "    # print(f\"   Fully cleaned BOLD saved to: {cleaned_bold_path}\")    \n",
    "    \n",
    "    # ==========================================\n",
    "    # 9. COMPUTE CONNECTIVITY MATRIX\n",
    "    # ==========================================\n",
    "    print(f\"\\n9. Computing {connectivity_kind} connectivity matrix...\")\n",
    "    \n",
    "    # Final check for NaN/Inf values before computing connectivity\n",
    "    if np.any(np.isnan(time_series_final)) or np.any(np.isinf(time_series_final)):\n",
    "        print(\"   Warning: Found NaN/Inf values in final time series. Removing...\")\n",
    "        # Remove timepoints with any NaN/Inf\n",
    "        valid_timepoints = ~(np.any(np.isnan(time_series_final), axis=1) | \n",
    "                           np.any(np.isinf(time_series_final), axis=1))\n",
    "        time_series_final = time_series_final[valid_timepoints, :]\n",
    "        print(f\"   Kept {time_series_final.shape[0]} valid timepoints\")\n",
    "    \n",
    "    # Check if we have enough timepoints\n",
    "    if time_series_final.shape[0] < 50:\n",
    "        raise ValueError(f\"Too few valid timepoints ({time_series_final.shape[0]}). Cannot compute reliable connectivity.\")\n",
    "    \n",
    "    connectivity_measure = ConnectivityMeasure(kind=connectivity_kind)\n",
    "    fc_matrix = connectivity_measure.fit_transform([time_series_final])[0]\n",
    "    \n",
    "    # Final check for NaN values in FC matrix\n",
    "    nan_count = np.sum(np.isnan(fc_matrix))\n",
    "    if nan_count > 0:\n",
    "        print(f\"   Warning: FC matrix contains {nan_count} NaN values. Replacing with 0.\")\n",
    "        fc_matrix = np.nan_to_num(fc_matrix, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "    \n",
    "    print(f\"   FC matrix shape: {fc_matrix.shape}\")\n",
    "    print(f\"   FC matrix range: [{np.nanmin(fc_matrix):.3f}, {np.nanmax(fc_matrix):.3f}]\")\n",
    "    print(f\"   FC matrix finite values: {np.sum(np.isfinite(fc_matrix))}/{fc_matrix.size}\")\n",
    "\n",
    "\n",
    "    if len(custom_atlas_labels) != fc_matrix.shape[0]:\n",
    "        raise ValueError(f\"Mismatch: atlas_labels ({len(custom_atlas_labels)}) vs fc_matrix size ({fc_matrix.shape[0]})\")\n",
    "\n",
    "    fc_df = pd.DataFrame(fc_matrix, index=custom_atlas_labels, columns=custom_atlas_labels)\n",
    "    print(custom_atlas_labels)\n",
    "    print(fc_df)\n",
    "    # ==========================================\n",
    "    # 10. QUALITY CONTROL METRICS\n",
    "    # ==========================================\n",
    "    print(f\"\\n10. Computing quality control metrics...\")\n",
    "    \n",
    "    qc_metrics = {\n",
    "        'mean_fd': confounds_df['framewise_displacement'].mean() if 'framewise_displacement' in confounds_df.columns else np.nan,\n",
    "        'mean_dvars': confounds_df['dvars'].mean() if 'dvars' in confounds_df.columns else np.nan,\n",
    "        'n_outliers': outlier_mask.sum(),\n",
    "        'outlier_fraction': outlier_mask.sum() / len(outlier_mask),\n",
    "        'n_timepoints': time_series_final.shape[0],\n",
    "        'n_regions': time_series_final.shape[1],\n",
    "        'mean_connectivity': np.nanmean(fc_matrix[np.triu_indices_from(fc_matrix, k=1)]),\n",
    "        'connectivity_sparsity': np.sum(np.abs(fc_matrix) > 0.1) / (fc_matrix.shape[0] * fc_matrix.shape[1]),\n",
    "        'n_finite_connections': np.sum(np.isfinite(fc_matrix)),\n",
    "        'n_nan_connections': np.sum(np.isnan(fc_matrix))\n",
    "    }\n",
    "    \n",
    "    print(\"   QC Metrics:\")\n",
    "    for key, value in qc_metrics.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"     {key}: {value:.3f}\")\n",
    "        else:\n",
    "            print(f\"     {key}: {value}\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # 11. SAVE RESULTS\n",
    "    # ==========================================\n",
    "    print(f\"\\n11. Saving results to {output_dir}...\")\n",
    "    \n",
    "    # Save FC matrix\n",
    "    np.save(os.path.join(output_dir, 'fc_matrix.npy'), fc_matrix)\n",
    "    fc_df.to_csv(os.path.join(output_dir, 'fc_matrix.csv'))\n",
    "    \n",
    "    # Save time series\n",
    "    np.save(os.path.join(output_dir, 'time_series_clean.npy'), time_series_final)\n",
    "    \n",
    "    # Save QC metrics\n",
    "    qc_df = pd.DataFrame([qc_metrics])\n",
    "    qc_df.to_csv(os.path.join(output_dir, 'qc_metrics.csv'), index=False)\n",
    "    \n",
    "    # Save region labels\n",
    "    labels_df = pd.DataFrame({'region_id': range(len(labels)), 'region_name': labels})\n",
    "    labels_df.to_csv(os.path.join(output_dir, 'atlas_labels.csv'), index=False)\n",
    "    params = {\n",
    "        'atlas_name': atlas_name,\n",
    "        'smoothing_fwhm': smoothing_fwhm,\n",
    "        'low_pass': low_pass,\n",
    "        'high_pass': high_pass,\n",
    "        't_r': t_r,\n",
    "        'fd_threshold': fd_threshold,\n",
    "        'dvars_threshold': dvars_threshold,\n",
    "        'denoising_strategy': denoising_strategy,\n",
    "        'connectivity_kind': connectivity_kind,\n",
    "        'n_confounds': len(confound_vars)\n",
    "    }\n",
    "    params_df = pd.DataFrame([params])\n",
    "    params_df.to_csv(os.path.join(output_dir, 'processing_params.csv'), index=False)\n",
    "    \n",
    "    # ==========================================\n",
    "    # 12. VISUALIZATION\n",
    "    # ==========================================\n",
    "    if plot_results:\n",
    "        print(f\"\\n12. Generating visualizations...\")\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # Plot 1: FC Matrix\n",
    "        im1 = axes[0, 0].imshow(fc_matrix, cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "        axes[0, 0].set_title(f'Functional Connectivity Matrix\\n({connectivity_kind})')\n",
    "        axes[0, 0].set_xlabel('Brain Regions')\n",
    "        axes[0, 0].set_ylabel('Brain Regions')\n",
    "        plt.colorbar(im1, ax=axes[0, 0])\n",
    "        \n",
    "        # Plot 2: Connectivity Distribution\n",
    "        fc_values = fc_matrix[np.triu_indices_from(fc_matrix, k=1)]\n",
    "        # Remove NaN/Inf values for plotting\n",
    "        fc_values_clean = fc_values[np.isfinite(fc_values)]\n",
    "        \n",
    "        if len(fc_values_clean) > 0:\n",
    "            axes[0, 1].hist(fc_values_clean, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "            axes[0, 1].set_title(f'Connectivity Distribution\\n({len(fc_values_clean)}/{len(fc_values)} finite values)')\n",
    "            axes[0, 1].set_xlabel('Correlation Coefficient')\n",
    "            axes[0, 1].set_ylabel('Frequency')\n",
    "            axes[0, 1].axvline(np.mean(fc_values_clean), color='red', linestyle='--', \n",
    "                              label=f'Mean: {np.mean(fc_values_clean):.3f}')\n",
    "            axes[0, 1].legend()\n",
    "        else:\n",
    "            axes[0, 1].text(0.5, 0.5, 'No finite connectivity values\\nto display', \n",
    "                           ha='center', va='center', transform=axes[0, 1].transAxes)\n",
    "            axes[0, 1].set_title('Connectivity Distribution - No Data')\n",
    "        \n",
    "        # Plot 3: Motion Parameters\n",
    "        if 'framewise_displacement' in confounds_df.columns:\n",
    "            time_points = np.arange(len(confounds_df))\n",
    "            axes[1, 0].plot(time_points, confounds_df['framewise_displacement'], \n",
    "                           color='blue', alpha=0.7)\n",
    "            axes[1, 0].axhline(fd_threshold, color='red', linestyle='--', \n",
    "                              label=f'Threshold: {fd_threshold}mm')\n",
    "            axes[1, 0].fill_between(time_points, 0, confounds_df['framewise_displacement'], \n",
    "                                   where=outlier_mask, color='red', alpha=0.3, \n",
    "                                   label=f'Outliers: {outlier_mask.sum()}')\n",
    "            axes[1, 0].set_title('Framewise Displacement')\n",
    "            axes[1, 0].set_xlabel('Time (TR)')\n",
    "            axes[1, 0].set_ylabel('FD (mm)')\n",
    "            axes[1, 0].legend()\n",
    "        \n",
    "        # Plot 4: Sample Time Series\n",
    "        n_show = min(10, time_series_final.shape[1])\n",
    "        for i in range(n_show):\n",
    "            axes[1, 1].plot(time_series_final[:, i] + i*3, alpha=0.7, linewidth=0.8)\n",
    "        axes[1, 1].set_title(f'Sample Time Series (first {n_show} regions)')\n",
    "        axes[1, 1].set_xlabel('Time (TR)')\n",
    "        axes[1, 1].set_ylabel('Signal (standardized) + offset')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, 'fc_analysis_summary.png'), \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Additional plot: Connectome visualization (if coordinates available)\n",
    "        if atlas_name == 'power_264':\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            plotting.plot_connectome(fc_matrix, coords, \n",
    "                                   edge_threshold='90%',\n",
    "                                   title=f'Functional Connectome - {atlas_name}')\n",
    "            plt.savefig(os.path.join(output_dir, 'connectome_plot.png'), \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "    \n",
    "    print(f\"\\n=== Pipeline Complete ===\")\n",
    "    print(f\"Results saved to: {output_dir}\")\n",
    "    \n",
    "    return {\n",
    "        'fc_matrix': fc_matrix,\n",
    "        'time_series': time_series_final,\n",
    "        'qc_metrics': qc_metrics,\n",
    "        'outlier_mask': outlier_mask,\n",
    "        'confounds_used': confound_vars\n",
    "    }\n",
    "\n",
    "#%%\n",
    "\n",
    "# ==========================================\n",
    "# USAGE EXAMPLE\n",
    "# ==========================================\n",
    "\n",
    "base_path = '/home/leas/Documents/PhD/'\n",
    "\n",
    "labels_file = f'{base_path}03_Data/atlas-hMRF/atlas-hMRF_dseg.tsv'\n",
    "if not os.path.exists(labels_file):\n",
    "    raise ValueError(f\"Labels file not found: {labels_file}\")\n",
    "\n",
    "labels_df = pd.read_csv(labels_file, sep=\"\\t\")  # Ensure tab separation\n",
    "\n",
    "# Extract labels from the second column named 'label'\n",
    "if \"label\" not in labels_df.columns:\n",
    "    raise ValueError(f\"Column 'label' not found in {labels_file}. Available columns: {labels_df.columns}\")\n",
    "\n",
    "labels = labels_df[\"label\"].tolist()  # Convert column to list\n",
    "# Define file paths (adjust these to your actual file paths)\n",
    "# bold_file = \"/media/leas/T7 Shield2/PNC_dataset/Bids_images/derivatives/sub-9293224671/func/sub-9293224671_task-bbl1restbold1124_run-1_echo-1_space-MNI152NLin2009cAsym_res-2_desc-preproc_bold.nii.gz\"\n",
    "# confounds_file = \"/media/leas/T7 Shield2/PNC_dataset/Bids_images/derivatives/sub-9293224671/func/sub-9293224671_task-bbl1restbold1124_run-1_desc-confounds_timeseries.tsv\"\n",
    "\n",
    "INPUT_DIR = '/media/leas/T7 Shield2/PNC_dataset/Bids_images/derivatives/'\n",
    "FILE_PATTERN = f'*/func/*_task-bbl1restbold1124_run-1_echo-1_space-MNI152NLin2009cAsym_res-2_desc-preproc_bold.nii.gz'\n",
    "\n",
    "\n",
    "search_pattern = os.path.join(INPUT_DIR, FILE_PATTERN)\n",
    "bold_file_list  = sorted(glob.glob(search_pattern))\n",
    "# bold_file_list \n",
    "\n",
    "base_output_dir = f'{base_path}/Projects/pnc/data/custom_preproc/'\n",
    "\n",
    "\n",
    "for bold_file in bold_file_list:\n",
    "    # Extract filename from full path\n",
    "    \n",
    "    filename = os.path.basename(bold_file)\n",
    "    match = re.search(r'(sub-[^_]+)', filename)\n",
    "    confounds_file = bold_file.replace('_echo-1_space-MNI152NLin2009cAsym_res-2_desc-preproc_bold.nii.gz', '_desc-confounds_timeseries.tsv')\n",
    "    subject_id = match.group(1)\n",
    "    # Create subject-specific output directory\n",
    "    subject_output_dir = os.path.join(base_output_dir, subject_id)\n",
    "\n",
    "    if os.path.exists(f\"{base_path}/Projects/pnc/data/custom_preproc/{subject_id}/fc_matrix.npy\"):\n",
    "        print(subject_id, \" already exists\")\n",
    "        continue\n",
    "    try:\n",
    "        # Run the complete pipeline with custom atlas\n",
    "        results = bold_to_fc_complete_pipeline(\n",
    "            bold_file=bold_file,\n",
    "            confounds_file=confounds_file,\n",
    "            atlas_name='custom',  # Use 'custom' for your own atlas\n",
    "            custom_atlas_file=f'{base_path}/03_Data/atlas-hMRF/space-MNI152NLin2009cAsym_atlas-hMRF_res-2_dseg.nii.gz',  # Path to your atlas\n",
    "            custom_atlas_labels=labels,  # Optional: custom labels\n",
    "            smoothing_fwhm=0,\n",
    "            low_pass=0.1, \n",
    "            high_pass=0.01,\n",
    "            t_r=3.0,\n",
    "            fd_threshold=0.6,\n",
    "            dvars_threshold=3.0,\n",
    "            denoising_strategy='12P+8PhysioCor+SpikeReg',  # NO GSR - Options: '6P', '24P', '24P+8PhysioCor+SpikeReg' (no GSR), '24P+8PhysioCor+GSR+SpikeReg' (with GSR)\n",
    "            standardize=True,\n",
    "            connectivity_kind='correlation',  # Options: 'correlation', 'partial correlation', 'covariance'\n",
    "            plot_results=True,\n",
    "            output_dir=subject_output_dir,\n",
    "            subject_id = subject_id\n",
    "        )\n",
    "    except:\n",
    "        \"Next\"\n",
    "        continue\n",
    "#%%\n",
    "\n",
    "# Access results\n",
    "fc_matrix = results['fc_matrix']\n",
    "time_series = results['time_series']\n",
    "qc_metrics = results['qc_metrics']\n",
    "\n",
    "print(f\"\\nFinal FC matrix shape: {fc_matrix.shape}\")\n",
    "print(f\"Mean connectivity: {qc_metrics['mean_connectivity']:.3f}\")\n",
    "\n",
    "# %%\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
